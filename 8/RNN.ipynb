{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.03316068649292\n",
      "Clip gradient :  2.626285924888852\n",
      "2.3404970169067383\n",
      "Clip gradient :  0.7095759849218488\n",
      "1.9489357471466064\n",
      "Clip gradient :  0.5791866418988671\n",
      "1.83073091506958\n",
      "Clip gradient :  0.35723572477479315\n",
      "1.4334876537322998\n",
      "Clip gradient :  0.28853635514587117\n",
      "1.1885817050933838\n",
      "Clip gradient :  3.077614004052836\n",
      "0.6803288459777832\n",
      "Clip gradient :  3.465572836200221\n",
      "0.3744478225708008\n",
      "Clip gradient :  0.33284903186139936\n",
      "0.294940710067749\n",
      "Clip gradient :  1.5264993843834198\n",
      "0.2093346118927002\n",
      "Clip gradient :  0.6694714282480253\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "# Peephole LSTM implementation\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size=5, out_size=5, bias=True):\n",
    "        super(LSTM, self).__init__()   \n",
    "  \n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = in_size\n",
    "        \n",
    "        self.sigm_activation_f = nn.Sigmoid()\n",
    "        self.sigm_activation_i = nn.Sigmoid()\n",
    "        self.sigm_activation_o = nn.Sigmoid()\n",
    "        \n",
    "        self.tanh_activation_c = nn.Tanh()\n",
    "        self.tanh_activation_h = nn.Tanh()\n",
    "        \n",
    "        self.f_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.f_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias )\n",
    "        \n",
    "        self.i_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.i_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        \n",
    "        self.o_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.o_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        \n",
    "        self.c_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=self.hidden_size, out_features=out_size)\n",
    "        \n",
    "    def forward(self, x, prev_c):\n",
    "        f = self.sigm_activation_f(self.f_x2h(x) + self.f_hidden(prev_c))\n",
    "        i = self.sigm_activation_i(self.i_x2h(x) + self.i_hidden(prev_c))\n",
    "        o = self.sigm_activation_o(self.o_x2h(x) + self.o_hidden(prev_c))\n",
    "        \n",
    "        c = f * prev_c + i * self.tanh_activation_c(self.c_x2h(x))\n",
    "        h = o * c\n",
    "        y = self.out(h)\n",
    "        \n",
    "        return y, c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "ds = WordDataSet(word=word)\n",
    "lstm = LSTM(in_size=ds.vec_size, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = Adam(lstm.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.8570785522461\n",
      "Clip gradient :  3.0038536452630624\n",
      "7.854757785797119\n",
      "Clip gradient :  11.249658075653167\n",
      "2.4592647552490234\n",
      "Clip gradient :  7.6202734546458295\n",
      "0.10059642791748047\n",
      "Clip gradient :  0.2255499947763327\n",
      "0.014684677124023438\n",
      "Clip gradient :  0.07922575121566285\n",
      "0.005322456359863281\n",
      "Clip gradient :  0.015420438977321628\n",
      "0.0023984909057617188\n",
      "Clip gradient :  0.005125523382784418\n",
      "0.0016279220581054688\n",
      "Clip gradient :  0.004057266860241546\n",
      "0.0012645721435546875\n",
      "Clip gradient :  0.0020207037678093864\n",
      "0.0010967254638671875\n",
      "Clip gradient :  0.001753066475971767\n",
      "0.0009679794311523438\n",
      "Clip gradient :  0.0013396732972110307\n",
      "0.0008792877197265625\n",
      "Clip gradient :  0.0012730166181135157\n",
      "0.000804901123046875\n",
      "Clip gradient :  0.0011036007590247\n",
      "0.0007429122924804688\n",
      "Clip gradient :  0.0009928215483807153\n",
      "0.0006971359252929688\n",
      "Clip gradient :  0.0009316020256877275\n",
      "0.00064849853515625\n",
      "Clip gradient :  0.0008776979038272071\n",
      "0.0006113052368164062\n",
      "Clip gradient :  0.0008236966150692002\n",
      "0.0005807876586914062\n",
      "Clip gradient :  0.0007776169073679826\n",
      "0.000537872314453125\n",
      "Clip gradient :  0.0007232130911620621\n",
      "0.000514984130859375\n",
      "Clip gradient :  0.0006955641269133429\n",
      "0.0004892349243164062\n",
      "Clip gradient :  0.0006569341981175786\n",
      "0.00046539306640625\n",
      "Clip gradient :  0.0006248283958381032\n",
      "0.00043773651123046875\n",
      "Clip gradient :  0.0005881432765751563\n",
      "0.0004253387451171875\n",
      "Clip gradient :  0.0005679886823182965\n",
      "0.000400543212890625\n",
      "Clip gradient :  0.0005286186735559231\n",
      "0.0003833770751953125\n",
      "Clip gradient :  0.0005222173686131273\n",
      "0.0003662109375\n",
      "Clip gradient :  0.0004935357604454738\n",
      "0.0003490447998046875\n",
      "Clip gradient :  0.00046891587779952234\n",
      "0.000331878662109375\n",
      "Clip gradient :  0.0004570340767554223\n",
      "0.00032138824462890625\n",
      "Clip gradient :  0.00043900076394337724\n",
      "0.000308990478515625\n",
      "Clip gradient :  0.00040706332187995605\n",
      "0.00029850006103515625\n",
      "Clip gradient :  0.0003929826395423704\n",
      "0.00028705596923828125\n",
      "Clip gradient :  0.00038075801020337615\n",
      "0.000270843505859375\n",
      "Clip gradient :  0.00036860767928642313\n",
      "0.00026416778564453125\n",
      "Clip gradient :  0.00035304038411093327\n",
      "0.0002536773681640625\n",
      "Clip gradient :  0.0003451079242707872\n",
      "0.00024318695068359375\n",
      "Clip gradient :  0.00033344514272886337\n",
      "0.00023555755615234375\n",
      "Clip gradient :  0.00032962533876475263\n",
      "0.000225067138671875\n",
      "Clip gradient :  0.0003199162686272326\n",
      "0.00021266937255859375\n",
      "Clip gradient :  0.00031713512682882453\n",
      "0.00020694732666015625\n",
      "Clip gradient :  0.00029755210751367453\n",
      "0.00020313262939453125\n",
      "Clip gradient :  0.00028946112162187747\n",
      "0.00019741058349609375\n",
      "Clip gradient :  0.00028969025528429744\n",
      "0.00018787384033203125\n",
      "Clip gradient :  0.0002784929438546046\n",
      "0.00018024444580078125\n",
      "Clip gradient :  0.0002785673830654577\n",
      "0.00016880035400390625\n",
      "Clip gradient :  0.0002551678688937753\n",
      "0.00016117095947265625\n",
      "Clip gradient :  0.00024347573126135965\n",
      "0.00016021728515625\n",
      "Clip gradient :  0.00023554124877862335\n",
      "0.00015163421630859375\n",
      "Clip gradient :  0.0002311197188688495\n",
      "0.00014781951904296875\n",
      "Clip gradient :  0.000218678229557069\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "for epoch in range(e_cnt):\n",
    "    cc = torch.zeros(lstm.in_size)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, cc = lstm(x, cc)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "cc = torch.zeros(lstm.in_size)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, cc = lstm(x, cc)\n",
    "    \n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_size=5,  out_size=5, bias=True):\n",
    "        super(GRU, self).__init__()  \n",
    "        \n",
    "        self.hidden_size = in_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.sigm_z = nn.Sigmoid()\n",
    "        self.sigm_r = nn.Sigmoid()\n",
    "        self.tan_h = nn.Tanh()\n",
    "        \n",
    "        self.z_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.z_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        \n",
    "        self.r_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.r_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        \n",
    "        self.h_x2h = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        self.h_hidden = nn.Linear(in_features=in_size, out_features=self.hidden_size, bias=bias)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=self.hidden_size, out_features=out_size)\n",
    "        \n",
    "    def forward(self, x, prev_h):\n",
    "        z = self.sigm_z(self.z_x2h(x) + self.z_hidden(prev_h))\n",
    "        r = self.sigm_r(self.r_x2h(x) + self.r_hidden(prev_h))\n",
    "\n",
    "        h = z * prev_h + (1. - z) * self.tan_h(self.h_x2h(x) + self.h_hidden(r * prev_h))\n",
    "        y = self.out(h)\n",
    "        \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "ds = WordDataSet(word=word)\n",
    "gru = GRU(in_size=ds.vec_size,  out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = SGD(gru.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.14236450195312\n",
      "Clip gradient :  5.055303000771596\n",
      "56.5948371887207\n",
      "Clip gradient :  9.271184235423982\n",
      "28.55231475830078\n",
      "Clip gradient :  11.662204908206084\n",
      "14.145013809204102\n",
      "Clip gradient :  9.473009859417527\n",
      "6.945394039154053\n",
      "Clip gradient :  6.867463626457366\n",
      "2.210543632507324\n",
      "Clip gradient :  3.2971812540620915\n",
      "0.3948345184326172\n",
      "Clip gradient :  0.7976592332775361\n",
      "0.10955142974853516\n",
      "Clip gradient :  0.10481674712130083\n",
      "0.052211761474609375\n",
      "Clip gradient :  0.06145053952224002\n",
      "0.032196044921875\n",
      "Clip gradient :  0.025888050208740264\n",
      "0.02506542205810547\n",
      "Clip gradient :  0.020045744843670797\n",
      "0.021053314208984375\n",
      "Clip gradient :  0.014902723855270165\n",
      "0.01864147186279297\n",
      "Clip gradient :  0.012741841281383092\n",
      "0.016928672790527344\n",
      "Clip gradient :  0.01155601729181755\n",
      "0.015563011169433594\n",
      "Clip gradient :  0.01054119207097949\n",
      "0.014435768127441406\n",
      "Clip gradient :  0.009750464301017461\n",
      "0.013478279113769531\n",
      "Clip gradient :  0.009100215227387125\n",
      "0.012643814086914062\n",
      "Clip gradient :  0.008536604513193049\n",
      "0.011908531188964844\n",
      "Clip gradient :  0.008042143992036021\n",
      "0.011263847351074219\n",
      "Clip gradient :  0.00760970631294567\n",
      "0.010682106018066406\n",
      "Clip gradient :  0.007220281622972398\n",
      "0.010161399841308594\n",
      "Clip gradient :  0.0068719953877011955\n",
      "0.009688377380371094\n",
      "Clip gradient :  0.00655571716857382\n",
      "0.009258270263671875\n",
      "Clip gradient :  0.006268383987895289\n",
      "0.008863449096679688\n",
      "Clip gradient :  0.006004477646046633\n",
      "0.008504867553710938\n",
      "Clip gradient :  0.005765199192656473\n",
      "0.008173942565917969\n",
      "Clip gradient :  0.005544506925940818\n",
      "0.007863998413085938\n",
      "Clip gradient :  0.005337185408603096\n",
      "0.0075817108154296875\n",
      "Clip gradient :  0.005148908442534242\n",
      "0.007313728332519531\n",
      "Clip gradient :  0.004969611143523613\n",
      "0.007065773010253906\n",
      "Clip gradient :  0.004804063132689363\n",
      "0.006838798522949219\n",
      "Clip gradient :  0.0046525008755593875\n",
      "0.006619453430175781\n",
      "Clip gradient :  0.004506087316193714\n",
      "0.0064144134521484375\n",
      "Clip gradient :  0.004368706008075749\n",
      "0.006224632263183594\n",
      "Clip gradient :  0.004241902973758163\n",
      "0.006045341491699219\n",
      "Clip gradient :  0.004121921612352283\n",
      "0.0058765411376953125\n",
      "Clip gradient :  0.004009114380312326\n",
      "0.005713462829589844\n",
      "Clip gradient :  0.003900015616834811\n",
      "0.005560874938964844\n",
      "Clip gradient :  0.0037977505221435223\n",
      "0.005417823791503906\n",
      "Clip gradient :  0.003701845168524407\n",
      "0.005279541015625\n",
      "Clip gradient :  0.003609158177620116\n",
      "0.005150794982910156\n",
      "Clip gradient :  0.003523012672914402\n",
      "0.005026817321777344\n",
      "Clip gradient :  0.003439787149999994\n",
      "0.004904747009277344\n",
      "Clip gradient :  0.003357708955495756\n",
      "0.0047969818115234375\n",
      "Clip gradient :  0.0032857238318212897\n",
      "0.004688262939453125\n",
      "Clip gradient :  0.0032127184233474975\n",
      "0.0045833587646484375\n",
      "Clip gradient :  0.0031420843533233054\n",
      "0.004486083984375\n",
      "Clip gradient :  0.0030770077575922226\n",
      "0.004393577575683594\n",
      "Clip gradient :  0.0030151880894693775\n",
      "0.004298210144042969\n",
      "Clip gradient :  0.0029506987580255463\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(gru.in_size).unsqueeze(0)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = gru(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=1)\n",
    "            \n",
    "\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "gru.eval()\n",
    "hh = torch.zeros(gru.in_size)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = gru(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
